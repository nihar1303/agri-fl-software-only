{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Federated Learning for Agricultural IoT - Analysis\n",
    "\n",
    "This notebook provides comprehensive analysis of the federated learning experiments.\n",
    "It loads results from different experiments and compares their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_results(results_dir='./results'):\n",
    "    \"\"\"Load all experiment results from the results directory\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    results_path = Path(results_dir)\n",
    "    \n",
    "    # Find all strategy result files\n",
    "    strategy_files = glob.glob(str(results_path / '*_strategy.json'))\n",
    "    \n",
    "    for strategy_file in strategy_files:\n",
    "        # Extract experiment name\n",
    "        exp_name = Path(strategy_file).stem.replace('_strategy', '')\n",
    "        \n",
    "        try:\n",
    "            # Load strategy results\n",
    "            with open(strategy_file, 'r') as f:\n",
    "                strategy_data = json.load(f)\n",
    "            \n",
    "            # Load history results\n",
    "            history_file = strategy_file.replace('_strategy.json', '_history.json')\n",
    "            if Path(history_file).exists():\n",
    "                with open(history_file, 'r') as f:\n",
    "                    history_data = json.load(f)\n",
    "            else:\n",
    "                history_data = {}\n",
    "            \n",
    "            # Load config\n",
    "            config_file = strategy_file.replace('_strategy.json', '_config.json')\n",
    "            if Path(config_file).exists():\n",
    "                with open(config_file, 'r') as f:\n",
    "                    config_data = json.load(f)\n",
    "            else:\n",
    "                config_data = {}\n",
    "            \n",
    "            results[exp_name] = {\n",
    "                'strategy': strategy_data,\n",
    "                'history': history_data,\n",
    "                'config': config_data\n",
    "            }\n",
    "            \n",
    "            print(f\"Loaded experiment: {exp_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {strategy_file}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load results\n",
    "results = load_experiment_results()\n",
    "print(f\"\\nLoaded {len(results)} experiments\")\n",
    "for name in results.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_performance_metrics(results):\n",
    "    \"\"\"Extract key performance metrics from results\"\"\"\n",
    "    \n",
    "    metrics_df = []\n",
    "    \n",
    "    for exp_name, data in results.items():\n",
    "        strategy_data = data.get('strategy', {})\n",
    "        history_data = data.get('history', {})\n",
    "        \n",
    "        training_history = strategy_data.get('training_history', {})\n",
    "        \n",
    "        # Extract metrics\n",
    "        global_accuracies = training_history.get('global_accuracy', [])\n",
    "        global_losses = training_history.get('global_loss', [])\n",
    "        client_accuracies = training_history.get('client_accuracies', [])\n",
    "        \n",
    "        if global_accuracies:\n",
    "            final_accuracy = global_accuracies[-1]\n",
    "            max_accuracy = max(global_accuracies)\n",
    "            convergence_round = len(global_accuracies)\n",
    "        else:\n",
    "            final_accuracy = max_accuracy = convergence_round = 0\n",
    "        \n",
    "        if global_losses:\n",
    "            final_loss = global_losses[-1]\n",
    "            min_loss = min(global_losses)\n",
    "        else:\n",
    "            final_loss = min_loss = 0\n",
    "        \n",
    "        # Calculate client accuracy variance (fairness metric)\n",
    "        if client_accuracies:\n",
    "            final_client_accs = client_accuracies[-1] if client_accuracies else []\n",
    "            if final_client_accs:\n",
    "                acc_std = np.std(final_client_accs)\n",
    "                acc_variance = np.var(final_client_accs)\n",
    "            else:\n",
    "                acc_std = acc_variance = 0\n",
    "        else:\n",
    "            acc_std = acc_variance = 0\n",
    "        \n",
    "        # Communication efficiency\n",
    "        communication_rounds = training_history.get('communication_rounds', convergence_round)\n",
    "        \n",
    "        metrics_df.append({\n",
    "            'Experiment': exp_name,\n",
    "            'Final Accuracy': final_accuracy,\n",
    "            'Max Accuracy': max_accuracy,\n",
    "            'Final Loss': final_loss,\n",
    "            'Min Loss': min_loss,\n",
    "            'Convergence Rounds': convergence_round,\n",
    "            'Accuracy Std': acc_std,\n",
    "            'Accuracy Variance': acc_variance,\n",
    "            'Communication Rounds': communication_rounds\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(metrics_df)\n",
    "\n",
    "# Extract and display metrics\n",
    "if results:\n",
    "    metrics_df = extract_performance_metrics(results)\n",
    "    print(\"Performance Comparison:\")\n",
    "    print(metrics_df.round(4))\n",
    "else:\n",
    "    print(\"No results found. Please run experiments first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_experiment_comparison(results):\n",
    "    \"\"\"Create comprehensive comparison plots\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('Federated Learning Experiment Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Global Accuracy Over Rounds\n",
    "    ax1 = axes[0, 0]\n",
    "    for exp_name, data in results.items():\n",
    "        accuracies = data.get('strategy', {}).get('training_history', {}).get('global_accuracy', [])\n",
    "        if accuracies:\n",
    "            rounds = list(range(1, len(accuracies) + 1))\n",
    "            ax1.plot(rounds, accuracies, marker='o', linewidth=2, label=exp_name.replace('_', ' '))\n",
    "    \n",
    "    ax1.set_title('Global Accuracy vs Rounds')\n",
    "    ax1.set_xlabel('Round')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Global Loss Over Rounds\n",
    "    ax2 = axes[0, 1]\n",
    "    for exp_name, data in results.items():\n",
    "        losses = data.get('strategy', {}).get('training_history', {}).get('global_loss', [])\n",
    "        if losses:\n",
    "            rounds = list(range(1, len(losses) + 1))\n",
    "            ax2.plot(rounds, losses, marker='s', linewidth=2, label=exp_name.replace('_', ' '))\n",
    "    \n",
    "    ax2.set_title('Global Loss vs Rounds')\n",
    "    ax2.set_xlabel('Round')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Final Accuracy Comparison\n",
    "    ax3 = axes[0, 2]\n",
    "    if 'metrics_df' in globals():\n",
    "        metrics_df.plot(x='Experiment', y='Final Accuracy', kind='bar', ax=ax3, legend=False, color='skyblue')\n",
    "        ax3.set_title('Final Accuracy Comparison')\n",
    "        ax3.set_ylabel('Accuracy')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 4: Accuracy Variance (Fairness)\n",
    "    ax4 = axes[1, 0]\n",
    "    for exp_name, data in results.items():\n",
    "        client_accs = data.get('strategy', {}).get('training_history', {}).get('client_accuracies', [])\n",
    "        if client_accs:\n",
    "            rounds = list(range(1, len(client_accs) + 1))\n",
    "            acc_stds = [np.std(accs) if accs else 0 for accs in client_accs]\n",
    "            ax4.plot(rounds, acc_stds, marker='^', linewidth=2, label=exp_name.replace('_', ' '))\n",
    "    \n",
    "    ax4.set_title('Client Accuracy Variance vs Rounds')\n",
    "    ax4.set_xlabel('Round')\n",
    "    ax4.set_ylabel('Accuracy Std Dev')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: Communication Efficiency\n",
    "    ax5 = axes[1, 1]\n",
    "    if 'metrics_df' in globals():\n",
    "        metrics_df.plot(x='Experiment', y='Communication Rounds', kind='bar', ax=ax5, legend=False, color='lightcoral')\n",
    "        ax5.set_title('Communication Rounds')\n",
    "        ax5.set_ylabel('Rounds')\n",
    "        ax5.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 6: Convergence Speed\n",
    "    ax6 = axes[1, 2]\n",
    "    convergence_data = []\n",
    "    exp_names = []\n",
    "    \n",
    "    for exp_name, data in results.items():\n",
    "        accuracies = data.get('strategy', {}).get('training_history', {}).get('global_accuracy', [])\n",
    "        if accuracies:\n",
    "            # Find round where accuracy reaches 90% of max\n",
    "            max_acc = max(accuracies)\n",
    "            target_acc = 0.9 * max_acc\n",
    "            convergence_round = next((i for i, acc in enumerate(accuracies) if acc >= target_acc), len(accuracies))\n",
    "            convergence_data.append(convergence_round + 1)\n",
    "            exp_names.append(exp_name.replace('_', ' '))\n",
    "    \n",
    "    if convergence_data:\n",
    "        ax6.bar(exp_names, convergence_data, color='lightgreen')\n",
    "        ax6.set_title('Convergence Speed (90% of Max Accuracy)')\n",
    "        ax6.set_ylabel('Rounds to Convergence')\n",
    "        ax6.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate plots\n",
    "plot_experiment_comparison(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_analysis(results):\n",
    "    \"\"\"Perform statistical analysis of results\"\"\"\n",
    "    \n",
    "    if not results or 'metrics_df' not in globals():\n",
    "        print(\"No data available for statistical analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== Statistical Analysis ===\\n\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"Summary Statistics:\")\n",
    "    print(metrics_df.describe().round(4))\n",
    "    print()\n",
    "    \n",
    "    # Best performing experiment\n",
    "    best_accuracy = metrics_df.loc[metrics_df['Final Accuracy'].idxmax()]\n",
    "    print(f\"Best Final Accuracy: {best_accuracy['Experiment']} ({best_accuracy['Final Accuracy']:.4f})\")\n",
    "    \n",
    "    best_convergence = metrics_df.loc[metrics_df['Convergence Rounds'].idxmin()]\n",
    "    print(f\"Fastest Convergence: {best_convergence['Experiment']} ({best_convergence['Convergence Rounds']} rounds)\")\n",
    "    \n",
    "    most_fair = metrics_df.loc[metrics_df['Accuracy Std'].idxmin()]\n",
    "    print(f\"Most Fair (lowest std): {most_fair['Experiment']} ({most_fair['Accuracy Std']:.4f})\")\n",
    "    print()\n",
    "    \n",
    "    # Improvement analysis\n",
    "    baseline_exp = None\n",
    "    for exp in metrics_df['Experiment']:\n",
    "        if 'baseline' in exp.lower() or 'fedavg' in exp.lower():\n",
    "            baseline_exp = exp\n",
    "            break\n",
    "    \n",
    "    if baseline_exp:\n",
    "        baseline_acc = metrics_df[metrics_df['Experiment'] == baseline_exp]['Final Accuracy'].iloc[0]\n",
    "        print(f\"Improvements over baseline ({baseline_exp}): {baseline_acc:.4f}\")\n",
    "        \n",
    "        for _, row in metrics_df.iterrows():\n",
    "            if row['Experiment'] != baseline_exp:\n",
    "                improvement = (row['Final Accuracy'] - baseline_acc) / baseline_acc * 100\n",
    "                print(f\"  {row['Experiment']}: {improvement:+.2f}%\")\n",
    "        print()\n",
    "    \n",
    "    # Correlation analysis\n",
    "    print(\"Correlation Analysis:\")\n",
    "    corr_cols = ['Final Accuracy', 'Convergence Rounds', 'Accuracy Std', 'Communication Rounds']\n",
    "    correlation_matrix = metrics_df[corr_cols].corr()\n",
    "    print(correlation_matrix.round(3))\n",
    "\n",
    "# Perform analysis\n",
    "statistical_analysis(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adaptive Participation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_adaptive_participation(results):\n",
    "    \"\"\"Analyze adaptive participation patterns\"\"\"\n",
    "    \n",
    "    adaptive_results = {name: data for name, data in results.items() \n",
    "                       if 'adaptive' in name.lower() or 'combined' in name.lower()}\n",
    "    \n",
    "    if not adaptive_results:\n",
    "        print(\"No adaptive participation experiments found\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== Adaptive Participation Analysis ===\\n\")\n",
    "    \n",
    "    for exp_name, data in adaptive_results.items():\n",
    "        print(f\"Experiment: {exp_name}\")\n",
    "        \n",
    "        training_history = data.get('strategy', {}).get('training_history', {})\n",
    "        participation_counts = training_history.get('participation_counts', {})\n",
    "        \n",
    "        if participation_counts:\n",
    "            client_ids = list(participation_counts.keys())\n",
    "            counts = list(participation_counts.values())\n",
    "            \n",
    "            print(f\"  Client participation distribution:\")\n",
    "            print(f\"    Mean: {np.mean(counts):.2f}\")\n",
    "            print(f\"    Std: {np.std(counts):.2f}\")\n",
    "            print(f\"    Min: {min(counts)}, Max: {max(counts)}\")\n",
    "            print(f\"    Fairness ratio (min/max): {min(counts)/max(counts):.3f}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "# Analyze adaptive participation\n",
    "analyze_adaptive_participation(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clustering(results):\n",
    "    \"\"\"Analyze clustering patterns and personalization effects\"\"\"\n",
    "    \n",
    "    clustering_results = {name: data for name, data in results.items() \n",
    "                         if 'cluster' in name.lower() or 'personalization' in name.lower() or 'combined' in name.lower()}\n",
    "    \n",
    "    if not clustering_results:\n",
    "        print(\"No clustering experiments found\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== Clustering and Personalization Analysis ===\\n\")\n",
    "    \n",
    "    for exp_name, data in clustering_results.items():\n",
    "        print(f\"Experiment: {exp_name}\")\n",
    "        \n",
    "        training_history = data.get('strategy', {}).get('training_history', {})\n",
    "        cluster_assignments = training_history.get('cluster_assignments', {})\n",
    "        \n",
    "        if cluster_assignments:\n",
    "            # Analyze cluster stability\n",
    "            rounds = sorted(cluster_assignments.keys())\n",
    "            \n",
    "            if len(rounds) > 1:\n",
    "                # Calculate cluster assignment changes\n",
    "                changes = 0\n",
    "                total_assignments = 0\n",
    "                \n",
    "                for i in range(1, len(rounds)):\n",
    "                    prev_assignments = cluster_assignments[rounds[i-1]]\n",
    "                    curr_assignments = cluster_assignments[rounds[i]]\n",
    "                    \n",
    "                    for client_id in prev_assignments:\n",
    "                        if client_id in curr_assignments:\n",
    "                            total_assignments += 1\n",
    "                            if prev_assignments[client_id] != curr_assignments[client_id]:\n",
    "                                changes += 1\n",
    "                \n",
    "                stability = 1 - (changes / total_assignments) if total_assignments > 0 else 1\n",
    "                print(f\"  Cluster stability: {stability:.3f}\")\n",
    "            \n",
    "            # Analyze final cluster distribution\n",
    "            if rounds:\n",
    "                final_assignments = cluster_assignments[rounds[-1]]\n",
    "                cluster_sizes = {}\n",
    "                for client_id, cluster_id in final_assignments.items():\n",
    "                    cluster_sizes[cluster_id] = cluster_sizes.get(cluster_id, 0) + 1\n",
    "                \n",
    "                print(f\"  Final cluster sizes: {dict(sorted(cluster_sizes.items()))}\")\n",
    "                \n",
    "                # Calculate cluster balance\n",
    "                sizes = list(cluster_sizes.values())\n",
    "                if sizes:\n",
    "                    balance = min(sizes) / max(sizes)\n",
    "                    print(f\"  Cluster balance (min/max): {balance:.3f}\")\n",
    "        \n",
    "        # Analyze personalization benefit\n",
    "        client_accuracies = training_history.get('client_accuracies', [])\n",
    "        if client_accuracies:\n",
    "            final_client_accs = client_accuracies[-1]\n",
    "            if final_client_accs:\n",
    "                print(f\"  Client accuracy range: [{min(final_client_accs):.4f}, {max(final_client_accs):.4f}]\")\n",
    "                print(f\"  Client accuracy std: {np.std(final_client_accs):.4f}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "# Analyze clustering\n",
    "analyze_clustering(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Recommendations and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(results):\n",
    "    \"\"\"Generate recommendations based on experimental results\"\"\"\n",
    "    \n",
    "    if not results or 'metrics_df' not in globals():\n",
    "        print(\"No data available for recommendations\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== Recommendations for Agricultural IoT Deployment ===\\n\")\n",
    "    \n",
    "    # Find best overall approach\n",
    "    best_overall = metrics_df.loc[metrics_df['Final Accuracy'].idxmax()]\n",
    "    most_fair = metrics_df.loc[metrics_df['Accuracy Std'].idxmin()]\n",
    "    fastest_convergence = metrics_df.loc[metrics_df['Convergence Rounds'].idxmin()]\n",
    "    \n",
    "    print(f\"1. **Best Overall Performance**: {best_overall['Experiment']}\")\n",
    "    print(f\"   - Achieves highest final accuracy: {best_overall['Final Accuracy']:.4f}\")\n",
    "    print(f\"   - Convergence time: {best_overall['Convergence Rounds']} rounds\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"2. **Most Fair Distribution**: {most_fair['Experiment']}\")\n",
    "    print(f\"   - Lowest accuracy variance: {most_fair['Accuracy Std']:.4f}\")\n",
    "    print(f\"   - Good for ensuring all farms benefit equally\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"3. **Fastest Convergence**: {fastest_convergence['Experiment']}\")\n",
    "    print(f\"   - Converges in: {fastest_convergence['Convergence Rounds']} rounds\")\n",
    "    print(f\"   - Best for time-critical deployments\")\n",
    "    print()\n",
    "    \n",
    "    # Practical recommendations\n",
    "    print(\"**Practical Deployment Recommendations:**\")\n",
    "    print(\"\")\n",
    "    print(\"1. **For Large-Scale Agricultural Networks:**\")\n",
    "    if 'combined' in best_overall['Experiment'].lower():\n",
    "        print(\"   - Use combined adaptive participation + clustering approach\")\n",
    "        print(\"   - Provides best balance of accuracy and fairness\")\n",
    "    print(\"\")\n",
    "    \n",
    "    print(\"2. **For Resource-Constrained Environments:**\")\n",
    "    print(\"   - Implement adaptive client selection to handle unreliable devices\")\n",
    "    print(\"   - Use compression techniques for communication efficiency\")\n",
    "    print(\"\")\n",
    "    \n",
    "    print(\"3. **For Heterogeneous Farm Conditions:**\")\n",
    "    print(\"   - Deploy clustered personalization for regional adaptations\")\n",
    "    print(\"   - Group farms by climate, soil type, and crop patterns\")\n",
    "    print(\"\")\n",
    "    \n",
    "    print(\"4. **For Data Privacy Concerns:**\")\n",
    "    print(\"   - Federated learning keeps sensitive farm data local\")\n",
    "    print(\"   - Additional differential privacy can be added if needed\")\n",
    "    print(\"\")\n",
    "    \n",
    "    print(\"**Key Implementation Guidelines:**\")\n",
    "    print(\"- Start with 3-5 clusters for regional personalization\")\n",
    "    print(\"- Use adaptive participation with 60-70% client sampling\")\n",
    "    print(\"- Implement FedProx (Î¼=0.01) for handling device heterogeneity\")\n",
    "    print(\"- Plan for 25-40 communication rounds for convergence\")\n",
    "    print(\"- Monitor client participation fairness and adjust weights accordingly\")\n",
    "\n",
    "# Generate recommendations\n",
    "generate_recommendations(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results for Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results_summary(results, output_file='experiment_summary.csv'):\n",
    "    \"\"\"Export results summary for inclusion in research paper\"\"\"\n",
    "    \n",
    "    if 'metrics_df' not in globals():\n",
    "        print(\"No metrics available to export\")\n",
    "        return\n",
    "    \n",
    "    # Create comprehensive summary\n",
    "    summary_df = metrics_df.copy()\n",
    "    \n",
    "    # Add additional metrics\n",
    "    summary_df['Accuracy Improvement %'] = 0.0\n",
    "    summary_df['Fairness Score'] = 1 / (1 + summary_df['Accuracy Std'])  # Higher is better\n",
    "    summary_df['Efficiency Score'] = summary_df['Max Accuracy'] / summary_df['Convergence Rounds']\n",
    "    \n",
    "    # Calculate improvements over baseline\n",
    "    baseline_acc = None\n",
    "    for _, row in summary_df.iterrows():\n",
    "        if 'baseline' in row['Experiment'].lower() or 'fedavg' in row['Experiment'].lower():\n",
    "            baseline_acc = row['Final Accuracy']\n",
    "            break\n",
    "    \n",
    "    if baseline_acc:\n",
    "        summary_df['Accuracy Improvement %'] = ((summary_df['Final Accuracy'] - baseline_acc) / baseline_acc * 100).round(2)\n",
    "    \n",
    "    # Reorder columns for clarity\n",
    "    column_order = [\n",
    "        'Experiment', 'Final Accuracy', 'Max Accuracy', 'Accuracy Improvement %',\n",
    "        'Final Loss', 'Convergence Rounds', 'Accuracy Std', 'Fairness Score',\n",
    "        'Efficiency Score', 'Communication Rounds'\n",
    "    ]\n",
    "    \n",
    "    summary_df = summary_df[column_order].round(4)\n",
    "    \n",
    "    # Export to CSV\n",
    "    summary_df.to_csv(output_file, index=False)\n",
    "    print(f\"Results summary exported to {output_file}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\nExperiment Summary:\")\n",
    "    print(summary_df)\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Export results\n",
    "if results:\n",
    "    summary = export_results_summary(results)\n",
    "else:\n",
    "    print(\"No results to export\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}